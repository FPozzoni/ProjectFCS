# -*- coding: utf-8 -*-
"""progetto-2021.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14hJ7oBzYAgf18boIlbQ1ghy_sniAuhJ7

You have to work on the files:
*  [Books](https://github.com/gdv/foundationsCS/raw/master/progetti/2021/Books.csv.gz)
*  [Book ratings](https://github.com/gdv/foundationsCS/raw/master/progetti/2021/Book-Ratings.csv.gz)
*  [Users](https://github.com/gdv/foundationsCS/raw/master/progetti/2021/Users.csv.gz)
*  [Goodbooks books](https://github.com/gdv/foundationsCS/raw/master/progetti/2021/goodbooks.csv.gz)
*  [Goodbooks ratings](https://github.com/gdv/foundationsCS/raw/master/progetti/2021/goodbooks-ratings.csv.gz)

### Notes

1.    It is mandatory to use GitHub for developing the project.
1.    The project must be a jupyter notebook.
1.    There is no restriction on the libraries that can be used, nor on the Python version.
1.    To read those files, you need to use the `encoding = 'latin-1'` option.
1.    All questions on the project **must** be asked in a public channel on [Zulip](https://focs.zulipchat.com), otherwise no  answer will be given.
"""

import pandas as pd
import numpy as np
import re

books = pd.read_csv("https://github.com/gdv/foundationsCS/raw/master/progetti/2021/Books.csv.gz", sep=';', encoding = 'latin-1')
bRatings = pd.read_csv("https://github.com/gdv/foundationsCS/raw/master/progetti/2021/Book-Ratings.csv.gz", sep=';', encoding = 'latin-1')
users = pd.read_csv("https://github.com/gdv/foundationsCS/raw/master/progetti/2021/Users.csv.gz", sep=';', encoding = 'latin-1')
goodBooks = pd.read_csv("https://github.com/gdv/foundationsCS/raw/master/progetti/2021/goodbooks.csv.gz", sep=',', encoding = 'latin-1')
gbRatings = pd.read_csv("https://github.com/gdv/foundationsCS/raw/master/progetti/2021/goodbooks-ratings.csv.gz", sep=',', encoding = 'latin-1')

books.head()

bRatings.head()

users.head()

goodBooks.head()

gbRatings.head()

goodBooks['original_publication_year']=goodBooks['original_publication_year'].astype(int)
goodBooks['isbn13']=goodBooks['isbn13'].astype(float).astype(int)

users["Location"]

"""### 1. Normalize the location field of *Users* dataset, splitting into city, region, country."""

# sfruttiamo come è costruita la colonna Users, con separatore la virgola
occ = users["Location"].str.split(",", n = 2, expand = True)
users["City"]= occ[0]
users["Region"]= occ[1]
users["Country"]= occ[2]
users

rows = users.Location.size
conta = 0
for line in users.Location:
  occ = re.finditer(',',line)
  count = 0
  for match in occ:
    count +=1
  if count!=2:
    conta +=1
print(f"percentuale diversi={round((conta/rows)*100,3)}%, numero={conta}, totale={rows}")

"""### 2. For each book in the *Books* dataset, compute its average rating."""

joinBookRatings = pd.merge(books, bRatings, on='ISBN')
joinBookRatings.head()

avgRatings = joinBookRatings.groupby(['ISBN','Book-Title','Book-Author','Year-Of-Publication','Publisher'], as_index=False)['Book-Rating'].mean()[['ISBN','Book-Title','Book-Author','Year-Of-Publication','Publisher','Book-Rating']]
avgRatings.rename({'Book-Rating': 'Media-Rating'}, axis=1, inplace=True)
avgRatings

"""### 3. For each book in the *GoodBooks* dataset, compute its average rating."""

goodBooks[['isbn', 'isbn13', 'authors', 'title', 'average_rating']].sort_values('average_rating',ascending=False)

goodBooks['rating_calcolato'] = (goodBooks['ratings_1']*1+goodBooks['ratings_2']*2+goodBooks['ratings_3']*3+goodBooks['ratings_4']*4+goodBooks['ratings_5']*5)/goodBooks['work_ratings_count']
goodBooks[['isbn', 'isbn13', 'authors', 'title', 'average_rating','rating_calcolato']].sort_values('rating_calcolato',ascending=False)

"""### 4. Merge together all rows sharing the same book title, author and publisher. We will call the resulting datset `merged books`. The books that have not been merged together will not appear in `merged books`."""

count = pd.DataFrame({'count'    : joinBookRatings.groupby( [ "Book-Title", "Book-Author", "Publisher"] ).size()}).reset_index()
merged_books = count[count['count'] >1]

merged_books

"""### 5. For each book in `merged books` compute its average rating.

The average is computed considering all books in `books` that have been merged.
"""

merged_books['Media-Rating'] = joinBookRatings.groupby(['Book-Title','Book-Author','Publisher'], as_index=False)['Book-Rating'].mean()['Book-Rating']

merged_books

"""### 6. For each book in `merged books` compute the minimum and maximum of the average ratings over all corresponding books in the `books` dataset.

Hence for each book in `merged books` we will have exactly two values (a minimum and a maximum)
"""

df1 = pd.DataFrame({'Media-Rating':joinBookRatings.groupby(['Book-Title','Book-Author','Publisher','ISBN'])['Book-Rating'].mean()}).reset_index()
merged_books['min'] = df1.groupby(['Book-Title','Book-Author','Publisher'], as_index=False)['Media-Rating'].min()['Media-Rating']
merged_books['max'] = df1.groupby(['Book-Title','Book-Author','Publisher'], as_index=False)['Media-Rating'].max()['Media-Rating']

merged_books

"""### 7. For each book in `goodbooks`, compute the list of its authors. Assuming that the number of reviews with a text (column `work_text_reviews_count`) is split equally among all authors, find for each authors the total number of reviews with a text. We will call this quantity the *shared number of reviews with a text*."""

authors = goodBooks[goodBooks.duplicated(['authors'])==False]

def remove(string):
    return " ".join(string.split())

myDict= {}
reg = 'a-zA-Z.\s\-\u00A1-\u00FF'
re_enum = re.compile('(['+reg+']+),?', re.UNICODE)
temp5 = authors.set_index('authors')['work_text_reviews_count'].to_dict()
for key,value in temp5.items():
  occ = re_enum.findall(key)
  if occ:
    j=0
    for j in range(len(occ)):
      if remove(occ[j]) not in myDict:
        myDict[remove(occ[j])] = int(value/len(occ))
      else:
        myDict[remove(occ[j])] = myDict[remove(occ[j])] + int(value/len(occ))

shared_number_of_reviews_with_text = pd.DataFrame(myDict.items(), columns=['Autori', 'shared_number_of_reviews_with_text'])

shared_number_of_reviews_with_text

#ATTENZIONE TUTTO IL CODICE SOPRA FUNZIONA MA 
#RIGA 'George Orwell, Erich Fromm, CelÃ¢l Ãster'
#viene divisa male in 
#['George Orwell', ' Erich Fromm', ' CelÃ¢l Ã', 'ster']

reg = 'a-zA-Z.\s\-\u00A1-\u00FF'
re_enum = re.compile('(['+reg+']+),?', re.UNICODE)
occ = re_enum.findall('George Orwell, Erich Fromm, CelÃ¢l Ãster')
print(occ)

"""### 8. For each year of publication, determine the author that has the largest value of the shared number of reviews with a text."""

myDict2= {}
reg = 'a-zA-Z.\s\-\u00A1-\u00FF'
re_enum = re.compile('(['+reg+']+),?', re.UNICODE)
temp6 = authors.set_index('authors')['original_publication_year'].to_dict()
for key,value in temp6.items():
  occ = re_enum.findall(key)
  if occ:
    j=0
    for j in range(len(occ)):
      if value not in myDict2:
        myDict2[value] = remove(occ[j])
      else:
        myDict2[value] = myDict2[value]+','+remove(occ[j])

myDict3 = {}
for anno,aut in myDict2.items():
  for autore,review in myDict.items():
    if re.search(autore,aut):
      if anno not in myDict3:
        myDict3[anno] = {'autore':autore,'review':review}
      else:
        if myDict3[anno]['review'] <= review:
          myDict3[anno]['review'] = review
year_reviews =pd.DataFrame.from_dict(myDict3,orient='index')
year_reviews.index.name = 'anno'
year_reviews.reset_index(level=['anno'])

"""### 9. Assuming that there are no errors in the ISBN fields, find the books in both datasets, and compute the difference of average rating according to the ratings and the goodratings datasets"""

libri = pd.DataFrame({'Rating_books':joinBookRatings.groupby(["ISBN"])['Book-Rating'].mean()}).reset_index()
good_libri = pd.DataFrame({'Rating_books':goodBooks.groupby(['isbn',"isbn13"])['average_rating'].mean()}).reset_index()
isbn_comuni = good_libri[good_libri.isbn.isin(libri.ISBN)]['isbn'].to_list()

row_list = []
for lib in isbn_comuni:
  val1 = libri[libri['ISBN']== lib]['Rating_books'].values[0]
  val2 = good_libri[good_libri['isbn']==lib]['Rating_books'].values[0]
  dict1 = {
      'isbn'             : lib,
      'Rating_books'     : val1,
      'Rating_goodbooks' : val2,
      'Difference'       : abs(val2-val1)
  }
  row_list.append(dict1)
df4 = pd.DataFrame(row_list, columns=['isbn','Rating_books','Rating_goodbooks','Difference'])
df4

"""### 10. Split the users dataset according to the age. One dataset contains the users with unknown age, one with age 0-14, one with age 15-24, one with age 25-34, and so on."""

users[users['Age'] == users['Age'].max()]

group_0_14 = users[users['Age']<15]
var_holder = {}
for i in range(1,24):
  var_holder["group_"+ str(i)+"5_"+str(i+1)+"4"] = users[(users['Age']>int(str(i)+"4")) & (users['Age']<int(str(i+1)+"5"))]
locals().update(var_holder)
#sono in dataframe con nome tipo group_25_34

"""### 11. Find the books that appear only in the goodbooks datasets."""

solo_in_goodbooks = goodBooks[~good_libri.isbn.isin(libri.ISBN)]

solo_in_goodbooks

"""### 12. Assuming that each pair (author, title) identifies a book, for each book find the number of times it appears in the books dataset. Which books appear the most times?"""

count_book = pd.DataFrame({'count': books.groupby( [ "Book-Title", "Book-Author"] ).size()}).reset_index()
count_book[count_book['count'] == count_book['count'].max()]

count_book

"""### 13. Find the author with the highest average rating according to the goodbooks datasets."""

myDict5= {}
reg = 'a-zA-Z.\s\-\u00A1-\u00FF'
re_enum = re.compile('(['+reg+']+),?', re.UNICODE)
temp5 = authors.set_index('authors')['average_rating'].to_dict()
for key,value in temp5.items():
  occ = re_enum.findall(key)
  if occ:
    for j in range(len(occ)):
      if remove(occ[j]) not in myDict5:
        myDict5[remove(occ[j])] = {'rating':value,'num_occ':len(occ)}
      else:
        myDict5[remove(occ[j])] = {'rating':myDict5[remove(occ[j])]['rating']+value,'num_occ':len(occ)}

author_max_rating = pd.DataFrame(myDict5.items(), columns=['Autori', 'rating_plus_count'])
normalized = pd.json_normalize(author_max_rating['rating_plus_count'])

author_max_rating = author_max_rating.join(normalized).drop(columns=['rating_plus_count'])
author_max_rating['rating']= author_max_rating['rating']/author_max_rating['num_occ']
author_max_rating.drop('num_occ', axis=1, inplace=True)
author_max_rating

author_max_rating.loc[author_max_rating['rating'].idxmax()]